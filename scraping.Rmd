---
title: "News Analytics final project"
author: "Koen Oosthoek (Github: koenoosthoek) & Abhiram Mohan (Github: Abhilearns2code)"
date: "`r format(Sys.time(), '%B %d, %Y | %H:%M:%S | %Z')`"
output:
  html_document:
    code_folding: show
    df_print: paged
    highlight: tango
    number_sections: no
    theme: cosmo
    toc: no
---
  
<style>
div.answer {background-color:#f3f0ff; border-radius: 5px; padding: 20px;}
</style>

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      eval = TRUE,
                      error = FALSE,
                      message = FALSE,
                      warning = FALSE,
                      comment = NA)
```

```{r results="asis"}
cat("
<style>
caption {
      color: black;
      font-weight: normal;
      font-size: 1.0em;
    }
</style>
")
```
***

```{r, include = T}
library(tidyverse)
library(xml2)
library(rvest)
library(tidytext)
library(stringr)
library(gtools)
library(stringr)
library(qpcR)
```

<br>

***

### Task 2 - Scraping newspaper headlines [14 points in total]

Use Selectorgadget and R to scrape the article headlines from https://www.theguardian.com/international. 

a) Provide the first 6 observations from the uncleaned vector of scraped headlines. [3 points]

```{r}
## classic way

# The Guardian (UK)
url_guard <- read_html("https://www.theguardian.com/international") # importing data of html
headlines_guard <- html_elements(url_guard, xpath = "//*[contains(@class, 'u-faux-block-link__overlay js-headline-text')]") # xpath
headlines_guard_raw <- html_text(headlines_guard) # headlines

# The NYTimes (US)
url_nytimes <- read_html("https://www.nytimes.com") # importing data of html
headlines_nytimes <- html_elements(url_nytimes, xpath = '//*[contains(concat(" ", @class, " " ), concat( " ", "e1lsht870", " " ))] | //*[contains(concat( " ", @class, " " ), concat(" ", "balancedHeadline", " " ))]')
headlines_nytimes_raw <- html_text(headlines_nytimes) # headlines

# The Times (UK)
url_times <- read_html("https://www.thetimes.co.uk/")
headlines_times <- html_elements(url_times, xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "Item-headline", " " ))]//*[contains(concat( " ", @class, " " ), concat( " ", "js-tracking", " " ))]')
headlines_times_raw <- html_text(headlines_times)

# WSJ (USA)
url_wsj <- read_html("https://www.wsj.com")
headlines_wsj <- html_elements(url_wsj, xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "WSJTheme--headline--nQ8J-FfZ", " " ))] | //*[contains(concat( " ", @class, " " ), concat( " ", "style--headline--2BxmSWrz", " " ))] | //*[contains(concat( " ", @class, " " ), concat( " ", "WSJTheme--stipple__link--2vFfymvf", " " ))] | //*[contains(concat( " ", @class, " " ), concat( " ", "WSJTheme--headlineText--He1ANr9C", " " ))]')
headlines_wsj_raw <- html_text(headlines_wsj)

# The Independent (UK)
url_independent <- read_html("https://www.independent.co.uk/")
headlines_independent <- html_elements(url_independent, xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "video-title", " " ))] | //*[contains(concat( " ", @class, " " ), concat( " ", "title", " " ))]')
headlines_independent_raw <- html_text(headlines_independent)

```

``` {r}
## with function
scrape <- function(x, y) {
  x = read_html(x)
  y = gsub('"', "'", y) # replace single quotation marks with double quotation marks to prevent syntax error
  hl = html_elements(x, xpath = y)
  hl_raw = html_text(hl)
  hl_raw = unique(hl_raw)
}

guardian <- scrape("https://www.theguardian.com/international", "//*[contains(@class, 'u-faux-block-link__overlay js-headline-text')]")

nytimes <- scrape("https://www.nytimes.com", '//*[contains(concat(" ", @class, " " ), concat( " ", "e1lsht870", " " ))] | //*[contains(concat( " ", @class, " " ), concat(" ", "balancedHeadline", " " ))]')

times <- scrape("https://www.thetimes.co.uk/", '//*[contains(concat( " ", @class, " " ), concat( " ", "Item-headline", " " ))]//*[contains(concat( " ", @class, " " ), concat( " ", "js-tracking", " " ))]')

wsj <- scrape("https://www.wsj.com", '//*[contains(concat( " ", @class, " " ), concat( " ", "WSJTheme--headline--nQ8J-FfZ", " " ))] | //*[contains(concat( " ", @class, " " ), concat( " ", "style--headline--2BxmSWrz", " " ))] | //*[contains(concat( " ", @class, " " ), concat( " ", "WSJTheme--stipple__link--2vFfymvf", " " ))] | //*[contains(concat( " ", @class, " " ), concat( " ", "WSJTheme--headlineText--He1ANr9C", " " ))]')

independent <- scrape("https://www.independent.co.uk/", '//*[contains(concat( " ", @class, " " ), concat( " ", "video-title", " " ))] | //*[contains(concat( " ", @class, " " ), concat( " ", "title", " " ))]')



lol <- qpcR:::cbind.na(guardian, nytimes, times, wsj, independent)

lol2 <- as.data.frame(lol)

lol2 <- pivot_longer(lol2, cols = everything())
```

<br>

b) Tidy the text data (e.g., remove irrelevant characters if there are any, and get rid of duplicates), compute the number of unique headings, and provide a random sample of 5 headings. [2 points]

```{r}
# The Guardian (UK)
hl_guard_df <- as.data.frame(headlines_guard_raw)
colnames(hl_guard_df)[colnames(hl_guard_df) == "headlines_guard_raw"] <- 'headline'
hl_guard_df$newspaper <- "guardian"

# NYtimes (USA)
hl_nytimes_df <- as.data.frame(headlines_nytimes_raw)
colnames(hl_nytimes_df)[colnames(hl_nytimes_df) == "headlines_nytimes_raw"] <- 'headline'
hl_nytimes_df$newspaper <- "nytimes" 

# The Times (UK)
hl_times_df <- as.data.frame(headlines_times_raw)
colnames(hl_times_df)[colnames(hl_times_df) == "headlines_times_raw"] <- 'headline'
hl_nytimes_df$newspaper <- "times" 

# WSJ (USA)
hl_wsj_df <- as.data.frame(headlines_wsj_raw)
colnames(hl_wsj_df)[colnames(hl_wsj_df) == "headlines_wsj_raw"] <- 'headline'
hl_wsj_df$newspaper <- "wsj" 




fra_join <- left_join(x = get_core(legislature = "fra"), # jointing both datasets
                      y = get_political(legislature = "fra"), 
                      by = "pageid")

left  <- full_join(x = hl_nytimes_df,
                   y = hl_guard_df, 
                   by = c("headline", "newspaper"))

```

<br>

c) Identify the 5 most frequent words in all headlines, excluding English stopwords. (Hint: use a string processing function from the `stringr` package to split up the headings word by word, and use an empty space, " ", as splitting pattern.) [2 points]

```{r}
# tokenizing headlines and ignoring stopwords
hl_df <- hl_df %>% 
  unnest_tokens(output = word, input = headlines_raw) %>% 
  anti_join(stop_words)

# counting words
hl_df_wc <- hl_df %>% 
  count(word, sort = TRUE)

# selecting the five most frequent headings
hl_df_wc <- hl_df_wc %>% 
  arrange(desc(n)) %>% 
  slice(1:5) 

# making a nice table
hl_df_wc %>%  
  dplyr::rename(Word = word, Frequency = n) %>% 
  knitr::kable(caption = "The five most frequent words in all headlines (excluding stopwords) ",
               table.attr = "style='width:25%;'") %>% 
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width =                                                 T, position = "center")
  

```

<br>

d) Develop an XPath expression that locates the set of links pointing to the articles behind the headings from the previous tasks and apply it to extract those links, storing them in a vector. List the first 5 links. *Note: The number of links might not be identical to the number of headings you extracted above. You may ignore minor differences.* [3 points]

```{r}
#XPath 
hl <- html_elements(url_g, xpath = "//a[@class='u-faux-block-link__overlay js-headline-text']/@href") # xpath to embedded links
hl_raw <- html_text(hl)
hl_raw <- as.vector(hl_raw)
hl_raw[1:5]
```

<br>

e) Provide polite code that downloads the article HTMLs to a local folder. Provide proof that the download was performed successfully by listing the first 5 files in the folder and giving the total number of files contained by the folder. Make sure that the folder itself is not synced to GitHub using `.gitignore`. [4 points]

```{r}
folder <- "articles/"
dir.create(folder)
filenames <- paste0("article_", 1:length(hl_raw), ".html")
filenames

for (i in 1:length(hl_raw)) {
  # only update, don't replace
    if (!file.exists(paste0(folder, filenames[i]))) {  
      tryCatch( 
        download.file(hl_raw[i], 
                      destfile = paste0(folder, filenames[i])),
        error = function(e) e
      )
      Sys.sleep(runif(1, 0, 1)) 
  
} }

list_files <- list.files(folder)
list_files <- mixedsort(list_files)
list_files[1:5] # 1 to 5
length(list_files) # all rows

# using gitignore in rstudio
```


